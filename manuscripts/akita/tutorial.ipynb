{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required inputs for Akita are:\n",
    "* binned Hi-C or Micro-C data stored in cooler format (https://github.com/mirnylab/cooler)\n",
    "* Genome FASTA file\n",
    "\n",
    "First, make sure you have a FASTA file available consistent with genome used for the coolers. Either add a symlink for a the data directory or download the machine learning friendly simplified version in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 19:46:45.034937: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-11 19:46:46.892218: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using TensorFlow device: /CPU:0\n",
      "Using TensorFlow device: /CPU:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'akita_train' from '/home/017448899/basenji/bin/akita_train.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ[\"PATH\"] = \"/usr/local/cuda-11.5/bin:\" + os.environ[\"PATH\"]\n",
    "os.environ[\"LD_LIBRARY_PATH\"] = \"/usr/local/cuda-11.5/lib64:\" + os.environ.get(\"LD_LIBRARY_PATH\", \"\")\n",
    "import shutil\n",
    "import subprocess\n",
    "from importlib import reload\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/017448899/basenji/bin')  # Replace with the actual path\n",
    "sys.path.append('/home/017448899/basenji/basenji')\n",
    "import dataset\n",
    "import akita_data\n",
    "import basenji_data_write\n",
    "import akita_train\n",
    "reload(dataset)\n",
    "reload(akita_data)\n",
    "reload(basenji_data_write)\n",
    "reload(akita_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n",
      "GPUs Available: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-11 19:46:59.344040: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "print(tf.__version__)\n",
    "print(\"GPUs Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/017448899/miniconda3/envs/basenji/bin/python\n",
      "2.13.1\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ModelSeq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[68], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(seqs_bed_file):\n\u001b[1;32m      9\u001b[0m     a \u001b[38;5;241m=\u001b[39m line\u001b[38;5;241m.\u001b[39msplit()\n\u001b[0;32m---> 10\u001b[0m     model_seqs\u001b[38;5;241m.\u001b[39mappend(\u001b[43mModelSeq\u001b[49m(a[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;28mint\u001b[39m(a[\u001b[38;5;241m1\u001b[39m]),\u001b[38;5;28mint\u001b[39m(a[\u001b[38;5;241m2\u001b[39m]),\u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m options\u001b[38;5;241m.\u001b[39mend_i \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m         options\u001b[38;5;241m.\u001b[39mend_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(model_seqs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ModelSeq' is not defined"
     ]
    }
   ],
   "source": [
    "import pysam\n",
    "\n",
    "# read FASTA\n",
    "fasta_file = \"./data/hg38.ml.fa\"\n",
    "fasta_open = pysam.Fastafile(fasta_file)\n",
    "seqs_bed_file = './data/1m/sequences.bed'\n",
    "model_seqs = []\n",
    "for line in open(seqs_bed_file):\n",
    "    a = line.split()\n",
    "    model_seqs.append(ModelSeq(a[0],int(a[1]),int(a[2]),None))\n",
    "\n",
    "    if options.end_i is None:\n",
    "        options.end_i = len(model_seqs)\n",
    "\n",
    "num_seqs = options.end_i - options.start_i\n",
    "seq_dna = fasta_open.fetch(mseq.chr, mseq.start, mseq.end)\n",
    "seq_dna = fetch_dna(fasta_open, mseq.chr, mseq_start, mseq_end)\n",
    "print(\"This is the sequence's length {}\".format(len(seq_dna)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-mers: ['ACGTAG', 'CGTAGG', 'GTAGGT', 'TAGGTC', 'AGGTCA', 'GGTCAG', 'GTCAGT', 'TCAGTT', 'CAGTTC', 'AGTTCG', 'GTTCGA', 'TTCGAT', 'TCGATC']\n",
      "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[-0.8302, -0.0339,  0.8400,  ..., -1.5183,  0.1905, -1.6725],\n",
      "         [-1.0396,  0.3682,  1.2651,  ..., -1.2533, -2.3157, -1.8425],\n",
      "         [-0.1801,  0.1782,  0.4828,  ..., -1.0035, -2.0247, -1.7128]],\n",
      "\n",
      "        [[-0.8531,  0.0451,  1.6959,  ..., -0.7248,  0.0168, -2.2951],\n",
      "         [ 0.6211, -0.8345,  0.4039,  ..., -1.1610, -1.3098, -1.3100],\n",
      "         [ 0.9651, -0.8113,  0.3069,  ..., -0.8097, -0.5686, -1.6427]],\n",
      "\n",
      "        [[ 0.5663, -0.8957,  0.3128,  ...,  0.6162, -0.8174, -2.1774],\n",
      "         [ 0.7426, -0.6788,  0.0847,  ...,  0.3084, -0.9915, -2.0305],\n",
      "         [ 0.3851, -0.6906,  0.2997,  ...,  0.2675, -0.6946, -1.8679]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.6784,  2.6782,  1.0214,  ..., -0.2528, -0.1694, -0.3810],\n",
      "         [-0.3693,  1.5446,  0.6190,  ...,  0.0420, -0.4617,  0.4870],\n",
      "         [-0.1759,  1.5771,  1.4095,  ...,  0.0629, -0.3647,  0.0721]],\n",
      "\n",
      "        [[ 1.1016,  1.4358, -1.0484,  ..., -1.8067, -0.5969,  1.2009],\n",
      "         [ 0.3516,  0.6820,  0.0752,  ..., -1.2243,  0.8860, -0.6962],\n",
      "         [ 0.4249,  0.6316, -0.6672,  ..., -1.0906,  0.6766, -0.6492]],\n",
      "\n",
      "        [[-0.3012,  0.9190,  0.6235,  ..., -1.6862,  0.7025, -0.7852],\n",
      "         [-0.7620,  1.2106,  0.8190,  ..., -1.3348,  0.1681, -1.0752],\n",
      "         [-0.6597,  1.1697,  0.7760,  ..., -1.5714,  0.0048, -1.0491]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 0.4178,  0.0482,  0.3650,  ..., -0.0343,  0.4806,  0.7142],\n",
      "        [ 0.4995,  0.1699,  0.1944,  ..., -0.1164,  0.6593,  0.6899],\n",
      "        [ 0.7292, -0.0491,  0.3475,  ...,  0.3895,  0.4072,  0.2962],\n",
      "        ...,\n",
      "        [ 0.2240,  0.4256,  0.0493,  ...,  0.1031,  0.5764, -0.1119],\n",
      "        [ 0.5730,  0.3242, -0.0698,  ..., -0.4731,  0.0660, -0.0768],\n",
      "        [ 0.0648,  0.1805,  0.0357,  ..., -0.3116,  0.2422, -0.4038]],\n",
      "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)\n",
      "Hidden states shape: torch.Size([13, 3, 768])\n",
      "Mean pooling embedding shape: torch.Size([13, 768])\n",
      "Max pooling embedding shape: torch.Size([13, 768])\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"zhihan1996/DNA_bert_6\")\n",
    "model = AutoModel.from_pretrained(\"zhihan1996/DNA_bert_6\")\n",
    "\n",
    "# Preprocess the DNA sequence into 6-mers\n",
    "def split_into_kmers(sequence, k=6):\n",
    "    return [sequence[i:i+k] for i in range(len(sequence) - k + 1)]\n",
    "\n",
    "dna = \"ACGTAGGTCAGTTCGATC\"\n",
    "kmers = split_into_kmers(dna, k=6)\n",
    "print(\"K-mers:\", kmers)\n",
    "\n",
    "# Tokenize the k-mers\n",
    "inputs = tokenizer(kmers, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "#print(\"Tokenized inputs:\", inputs)\n",
    "\n",
    "# Pass through the model\n",
    "outputs = model(**inputs)\n",
    "print(outputs)\n",
    "hidden_states = outputs.last_hidden_state  # Shape: [batch_size, sequence_length, hidden_size]\n",
    "print(\"Hidden states shape:\", hidden_states.shape)\n",
    "\n",
    "# Mean pooling\n",
    "embedding_mean = torch.mean(hidden_states, dim=1)  # Pool across sequence length\n",
    "print(\"Mean pooling embedding shape:\", embedding_mean.shape)  # Expect: [batch_size, hidden_size]\n",
    "\n",
    "# Max pooling\n",
    "embedding_max = torch.max(hidden_states, dim=1)[0]  # Pool across sequence length\n",
    "print(\"Max pooling embedding shape:\", embedding_max.shape)  # Expect: [batch_size, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isfile('./data/hg38.ml.fa'):\n",
    "    print('downloading hg38.ml.fa')\n",
    "    subprocess.call('curl -o ./data/hg38.ml.fa.gz https://storage.googleapis.com/basenji_barnyard2/hg38.ml.fa.gz', shell=True)\n",
    "    subprocess.call('gunzip ./data/hg38.ml.fa.gz', shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download a few Micro-C datasets, processed using distiller (https://github.com/mirnylab/distiller-nf), binned to 2048bp, and iteratively corrected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('./data/coolers'):\n",
    "    os.mkdir('./data/coolers')\n",
    "if not os.path.isfile('./data/coolers/HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool'):\n",
    "    subprocess.call('curl -o ./data/coolers/HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool'+\n",
    "            ' https://storage.googleapis.com/basenji_hic/tutorials/coolers/HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool', shell=True)\n",
    "    subprocess.call('curl -o ./data/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool'+\n",
    "            ' https://storage.googleapis.com/basenji_hic/tutorials/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool', shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool\n",
      "HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool\n"
     ]
    }
   ],
   "source": [
    "ls ./data/coolers/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write out these cooler files and labels to a samples table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = [['index','identifier','file','clip','sum_stat','description']]\n",
    "lines.append(['0', 'HFF', './data/coolers/HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool', '2', 'sum', 'HFF'])\n",
    "lines.append(['1', 'H1hESC', './data/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool', '2', 'sum', 'H1hESC'])\n",
    "\n",
    "samples_out = open('data/microc_cools.txt', 'w')\n",
    "for line in lines:\n",
    "    print('\\t'.join(line), file=samples_out)\n",
    "samples_out.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to choose genomic sequences to form batches for stochastic gradient descent, divide them into training/validation/test sets, and construct TFRecords to provide to downstream programs.\n",
    "\n",
    "The script [akita_data.py](https://github.com/calico/basenji/blob/master/bin/akita_data.py) implements this procedure.\n",
    "\n",
    "The most relevant options here are:\n",
    "\n",
    "| Option/Argument | Value | Note |\n",
    "|:---|:---|:---|\n",
    "| --sample | 0.1 | Down-sample the genome to 10% to speed things up here. |\n",
    "| -g | data/hg38_gaps_binsize2048_numconseq10.bed | Dodge large-scale unmappable regions determined from filtered cooler bins. |\n",
    "| -l | 1048576 | Sequence length. |\n",
    "| --crop | 65536 | Crop edges of matrix so loss is only computed over the central region. |\n",
    "| --local | True | Run locally, as opposed to on a SLURM scheduler. |\n",
    "| -o | data/1m | Output directory |\n",
    "| -p | 8 | Uses multiple concourrent processes to read/write. |\n",
    "| -t | .1 | Hold out 10% sequences for testing. |\n",
    "| -v | .1 | Hold out 10% sequences for validation. |\n",
    "| -w | 2048 | Pool the nucleotide-resolution values to 2048 bp bins. |\n",
    "| fasta_file| data/hg38.ml.fa | FASTA file to extract sequences from. |\n",
    "| targets_file | data/microc_cools.txt | Target table with cooler paths. |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: make sure to export BASENJIDIR as outlined in the basenji installation tips \n",
    "(https://github.com/calico/basenji/tree/master/#installation). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isdir('data/1m'):\n",
    "    shutil.rmtree('data/1m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 16] Device or resource busy: '.nfs0000000012125faf0001ef5b'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/1m\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mshutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrmtree\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata/1m\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/basenji/lib/python3.8/shutil.py:718\u001b[0m, in \u001b[0;36mrmtree\u001b[0;34m(path, ignore_errors, onerror)\u001b[0m\n\u001b[1;32m    716\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    717\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(fd)):\n\u001b[0;32m--> 718\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    719\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    720\u001b[0m             os\u001b[38;5;241m.\u001b[39mrmdir(path)\n",
      "File \u001b[0;32m~/miniconda3/envs/basenji/lib/python3.8/shutil.py:655\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    654\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msamestat(orig_st, os\u001b[38;5;241m.\u001b[39mfstat(dirfd)):\n\u001b[0;32m--> 655\u001b[0m         \u001b[43m_rmtree_safe_fd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirfd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43monerror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    657\u001b[0m             os\u001b[38;5;241m.\u001b[39mrmdir(entry\u001b[38;5;241m.\u001b[39mname, dir_fd\u001b[38;5;241m=\u001b[39mtopfd)\n",
      "File \u001b[0;32m~/miniconda3/envs/basenji/lib/python3.8/shutil.py:675\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    673\u001b[0m     os\u001b[38;5;241m.\u001b[39munlink(entry\u001b[38;5;241m.\u001b[39mname, dir_fd\u001b[38;5;241m=\u001b[39mtopfd)\n\u001b[1;32m    674\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m--> 675\u001b[0m     \u001b[43monerror\u001b[49m\u001b[43m(\u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfullname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/basenji/lib/python3.8/shutil.py:673\u001b[0m, in \u001b[0;36m_rmtree_safe_fd\u001b[0;34m(topfd, path, onerror)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    672\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 673\u001b[0m         \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlink\u001b[49m\u001b[43m(\u001b[49m\u001b[43mentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdir_fd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopfd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m    675\u001b[0m         onerror(os\u001b[38;5;241m.\u001b[39munlink, fullname, sys\u001b[38;5;241m.\u001b[39mexc_info())\n",
      "\u001b[0;31mOSError\u001b[0m: [Errno 16] Device or resource busy: '.nfs0000000012125faf0001ef5b'"
     ]
    }
   ],
   "source": [
    "if os.path.isdir('data/1m_onehot'):\n",
    "    shutil.rmtree('data/1m_onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contigs divided into\n",
      " Train:   526 contigs, 2136321418 nt (0.8009)\n",
      " Valid:    56 contigs,  265659136 nt (0.0996)\n",
      " Test:     61 contigs,  265472768 nt (0.0995)\n",
      "writing sequences to BED\n",
      "/home/017448899/basenji/bin/akita_data_read.py --crop 16384 -d 2 -k 0 -w 2048 --clip 2.000000 --as_obsexp ./data/coolers/HFF_hg38_4DNFIP5EUOFX.mapq_30.2048.cool ./data/1m/sequences.bed ./data/1m/seqs_cov/0.h5\n",
      "/home/017448899/basenji/bin/akita_data_read.py --crop 16384 -d 2 -k 0 -w 2048 --clip 2.000000 --as_obsexp ./data/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool ./data/1m/sequences.bed ./data/1m/seqs_cov/1.h5\n",
      "/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/cooltools/lib/numutils.py:1376: RuntimeWarning: invalid value encountered in divide\n",
      "  val_cur = ar_cur / armask_cur\n",
      "/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/cooltools/lib/numutils.py:1376: RuntimeWarning: invalid value encountered in divide\n",
      "  val_cur = ar_cur / armask_cur\n",
      "WARNING: ./data/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool >50% bins filtered, check:  chr4:143906816-144168960. \n",
      "WARNING: ./data/coolers/H1hESC_hg38_4DNFI1O6IL1Q.mapq_30.2048.cool >50% bins filtered, check:  chr11:88791040-89053184. \n",
      "These are the fold labels ['train', 'valid', 'test']\n",
      "This is the start0\n",
      "THis is the end 128\n",
      "This is the start5976\n",
      "THis is the end 6104\n",
      "This is the start6354\n",
      "THis is the end 6482\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 0 -e 128 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-0.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 128 -e 256 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-1.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 256 -e 384 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-2.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 384 -e 512 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-3.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 512 -e 640 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-4.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 640 -e 768 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-5.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 768 -e 896 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-6.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 896 -e 1024 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-7.tfr\n",
      "EOF\n",
      "Submitted batch job 108200\n",
      "Submitted batch job 108201\n",
      "Submitted batch job 108202\n",
      "Submitted batch job 108203\n",
      "Submitted batch job 108204\n",
      "Submitted batch job 108205\n",
      "Submitted batch job 108206\n",
      "Submitted batch job 108207\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1024 -e 1152 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-8.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1152 -e 1280 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-9.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1280 -e 1408 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-10.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1408 -e 1536 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-11.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1536 -e 1664 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-12.tfr\n",
      "EOF\n",
      "Submitted batch job 108208\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1664 -e 1792 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-13.tfr\n",
      "EOF\n",
      "Submitted batch job 108209\n",
      "Submitted batch job 108210\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1792 -e 1920 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-14.tfr\n",
      "EOF\n",
      "Submitted batch job 108211\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 1920 -e 2048 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-15.tfr\n",
      "EOF\n",
      "Submitted batch job 108212\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2048 -e 2176 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-16.tfr\n",
      "EOF\n",
      "Submitted batch job 108213\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2176 -e 2304 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-17.tfr\n",
      "EOF\n",
      "Submitted batch job 108214\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2304 -e 2432 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-18.tfr\n",
      "EOF\n",
      "Submitted batch job 108215\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2432 -e 2560 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-19.tfr\n",
      "EOF\n",
      "Submitted batch job 108216\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2560 -e 2688 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-20.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2688 -e 2816 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-21.tfr\n",
      "EOF\n",
      "Submitted batch job 108217\n",
      "Submitted batch job 108218\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2816 -e 2944 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-22.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 2944 -e 3072 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-23.tfr\n",
      "EOF\n",
      "Submitted batch job 108219\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3072 -e 3200 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-24.tfr\n",
      "EOF\n",
      "Submitted batch job 108220\n",
      "Submitted batch job 108221\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3200 -e 3328 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-25.tfr\n",
      "EOF\n",
      "Submitted batch job 108222\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3328 -e 3456 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-26.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3456 -e 3584 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-27.tfr\n",
      "EOF\n",
      "Submitted batch job 108223\n",
      "Submitted batch job 108224\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3584 -e 3712 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-28.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3712 -e 3840 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-29.tfr\n",
      "EOF\n",
      "Submitted batch job 108225\n",
      "Submitted batch job 108226\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3840 -e 3968 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-30.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 3968 -e 4096 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-31.tfr\n",
      "EOF\n",
      "Submitted batch job 108227\n",
      "Submitted batch job 108228\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4096 -e 4224 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-32.tfr\n",
      "EOF\n",
      "Submitted batch job 108229\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4224 -e 4352 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-33.tfr\n",
      "EOF\n",
      "Submitted batch job 108230\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4352 -e 4480 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-34.tfr\n",
      "EOF\n",
      "Submitted batch job 108231\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4480 -e 4608 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-35.tfr\n",
      "EOF\n",
      "Submitted batch job 108232\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4608 -e 4736 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-36.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4736 -e 4864 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-37.tfr\n",
      "EOF\n",
      "Submitted batch job 108233\n",
      "Submitted batch job 108234\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4864 -e 4992 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-38.tfr\n",
      "EOF\n",
      "Submitted batch job 108235\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 4992 -e 5120 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-39.tfr\n",
      "EOF\n",
      "Submitted batch job 108236\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5120 -e 5248 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-40.tfr\n",
      "EOF\n",
      "Submitted batch job 108237\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5248 -e 5376 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-41.tfr\n",
      "EOF\n",
      "Submitted batch job 108238\n",
      "Submitted batch job 108239\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5376 -e 5504 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-42.tfr\n",
      "EOF\n",
      "Submitted batch job 108240\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5504 -e 5632 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-43.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5632 -e 5760 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-44.tfr\n",
      "EOF\n",
      "Submitted batch job 108241\n",
      "Submitted batch job 108242\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5760 -e 5888 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-45.tfr\n",
      "EOF\n",
      "Submitted batch job 108243\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5888 -e 5976 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/train-46.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 5976 -e 6104 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/valid-0.tfr\n",
      "EOF\n",
      "Submitted batch job 108244\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 6104 -e 6232 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/valid-1.tfr\n",
      "EOF\n",
      "Submitted batch job 108245\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 6232 -e 6354 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/valid-2.tfr\n",
      "EOF\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 6354 -e 6482 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/test-0.tfr\n",
      "EOF\n",
      "Submitted batch job 108246\n",
      "Submitted batch job 108247\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 6482 -e 6610 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/test-1.tfr\n",
      "EOF\n",
      "Submitted batch job 108248\n",
      "sbatch --partition=compute --nodes=1 --ntasks=1 --cpus-per-task=16 --mem=96G --time=23:00:00<<EOF\n",
      "#!/bin/bash\n",
      "python /home/017448899/basenji/bin/basenji_data_write.py -s 6610 -e 6732 ./data/hg38.ml.fa ./data/1m/sequences.bed ./data/1m/seqs_cov ./data/1m/tfrecords/test-2.tfr\n",
      "EOF\n",
      "Submitted batch job 108249\n",
      "Submitted batch job 108250\n",
      "Submitted batch job 108251\n",
      "Submitted batch job 108252\n"
     ]
    }
   ],
   "source": [
    "! /home/017448899/basenji/bin/akita_data.py --sample 0.75 -g ./data/hg38_gaps_binsize2048_numconseq10.bed -l 262144 --crop 16384 --local -o ./data/1m --as_obsexp -p 8 -t .1 -v .1 -w 2048 --snap 2048 --stride_train 262144 --stride_test 524288 ./data/hg38.ml.fa ./data/microc_cools.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data for training is now saved in data/1m as tfrecords (for training, validation, and testing), where *contigs.bed* contains the original large contiguous regions from which training sequences were taken, and *sequences.bed* contains the train/valid/test sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5 test\n",
      "     83 train\n",
      "      5 valid\n"
     ]
    }
   ],
   "source": [
    "! cut -f4 data/1m/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      5 test\n",
      "     83 train\n",
      "      5 valid\n"
     ]
    }
   ],
   "source": [
    "! cut -f4 data/1m/sequences.bed | sort | uniq -c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr8\t97886208\t97947648\ttrain\n",
      "chr8\t22872064\t22933504\ttrain\n",
      "chr1\t180869120\t180930560\ttrain\n"
     ]
    }
   ],
   "source": [
    "! head -n3 data/1m/sequences.bed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chr8\t97886208\t97947648\ttrain\n",
      "chr8\t22872064\t22933504\ttrain\n",
      "chr1\t180869120\t180930560\ttrain\n"
     ]
    }
   ],
   "source": [
    "! head -n3 data/1m/sequences.bed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train a model!\n",
    "\n",
    "(Note: for training production-level models, please remove the --sample option when generating tfrecords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters json to have only two targets\n",
    "import json\n",
    "params_file   = './params.json'\n",
    "with open(params_file) as params_file:\n",
    "    params_tutorial = json.load(params_file)   \n",
    "params_tutorial['model']['head_hic'][-1]['units'] =2\n",
    "with open('./data/1m/params_tutorial.json','w') as params_tutorial_file:\n",
    "    json.dump(params_tutorial,params_tutorial_file) \n",
    "    \n",
    "### note that training with default parameters requires GPU with >12Gb RAM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify model parameters json to have only two targets\n",
    "import json\n",
    "params_file   = './params.json'\n",
    "with open(params_file) as params_file:\n",
    "    params_tutorial = json.load(params_file)   \n",
    "params_tutorial['model']['head_hic'][-1]['units'] =2\n",
    "with open('./data/1m_onehot/params_tutorial.json','w') as params_tutorial_file:\n",
    "    json.dump(params_tutorial,params_tutorial_file) \n",
    "    \n",
    "### note that training with default parameters requires GPU with >12Gb RAM ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-08 02:08:10.059678: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-08 02:08:11.704724: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "./data/1m/params_tutorial.json\n",
      "HI\n",
      "\n",
      "Dataset Parameters:\n",
      "seq_depth: 4\n",
      "seq_1hot: True\n",
      "target_length: 210\n",
      "num_targets: 2\n",
      "pool_width: 2048\n",
      "\n",
      "HELLO\n",
      "2025-03-08 02:08:14.023931: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "ðŸ”¹ Original sequence shape: (None,)\n",
      "âœ… Fixed sequence shape: (62, 768)\n",
      "Loading dataset from: ./data/1m\n",
      "Features shape: (2, 62, 768)\n",
      "Labels shape: (2, 210, 2)\n",
      "x shape: (2, 62, 768)\n",
      "y shape: (2, 210, 2)\n",
      "HI\n",
      "\n",
      "Dataset Parameters:\n",
      "seq_depth: 4\n",
      "seq_1hot: True\n",
      "target_length: 210\n",
      "num_targets: 2\n",
      "pool_width: 2048\n",
      "\n",
      "HELLO\n",
      "ðŸ”¹ Original sequence shape: (None,)\n",
      "âœ… Fixed sequence shape: (62, 768)\n",
      "Loading dataset from: ./data/1m\n",
      "Features shape: (2, 62, 768)\n",
      "Labels shape: (2, 210, 2)\n",
      "We're here helloooooo\n",
      "âœ… Model input shape before first layer: (None, 62, 768)\n",
      "âœ… Shape after embedding_projection: (None, 62, 64)\n",
      "âœ… Shape after first BiLSTM: (None, 62, 256)\n",
      "âœ… Shape after second BiLSTM: (None, 62, 256)\n",
      "âœ… Shape after upsampling: (None, 372, 256)\n",
      "This is cropping83\n",
      "This is inputs shapeeeeeeeeeeeeeeeeeeeeeeee (None, 186, 186, 48)\n",
      "This is triu_tup (array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "       17, 17, 17, 18, 18, 19]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  8,  9, 10, 11,\n",
      "       12, 13, 14, 15, 16, 17, 18, 19,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 12, 13, 14, 15, 16, 17, 18, 19, 13, 14, 15, 16, 17,\n",
      "       18, 19, 14, 15, 16, 17, 18, 19, 15, 16, 17, 18, 19, 16, 17, 18, 19,\n",
      "       17, 18, 19, 18, 19, 19]))\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " sequence (InputLayer)       [(None, 62, 768)]            0         []                            \n",
      "                                                                                                  \n",
      " embedding_projection (Dens  (None, 62, 64)               49216     ['sequence[0][0]']            \n",
      " e)                                                                                               \n",
      "                                                                                                  \n",
      " bidirectional_lstm1 (Bidir  (None, 62, 256)              197632    ['embedding_projection[0][0]']\n",
      " ectional)                                                                                        \n",
      "                                                                                                  \n",
      " bidirectional_lstm2 (Bidir  (None, 62, 256)              394240    ['bidirectional_lstm1[0][0]'] \n",
      " ectional)                                                                                        \n",
      "                                                                                                  \n",
      " upsample_1d (UpSampling1D)  (None, 372, 256)             0         ['bidirectional_lstm2[0][0]'] \n",
      "                                                                                                  \n",
      " stochastic_shift (Stochast  (None, 372, 256)             0         ['upsample_1d[0][0]']         \n",
      " icShift)                                                                                         \n",
      "                                                                                                  \n",
      " re_lu (ReLU)                (None, 372, 256)             0         ['stochastic_shift[0][0]']    \n",
      "                                                                                                  \n",
      " conv1d (Conv1D)             (None, 372, 64)              114688    ['re_lu[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization (Batch  (None, 372, 64)              256       ['conv1d[0][0]']              \n",
      " Normalization)                                                                                   \n",
      "                                                                                                  \n",
      " max_pooling1d (MaxPooling1  (None, 186, 64)              0         ['batch_normalization[0][0]'] \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu_1 (ReLU)              (None, 186, 64)              0         ['max_pooling1d[0][0]']       \n",
      "                                                                                                  \n",
      " conv1d_1 (Conv1D)           (None, 186, 64)              20480     ['re_lu_1[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (Bat  (None, 186, 64)              256       ['conv1d_1[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_2 (ReLU)              (None, 186, 64)              0         ['batch_normalization_1[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_2 (Conv1D)           (None, 186, 48)              9216      ['re_lu_2[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_2 (Bat  (None, 186, 48)              192       ['conv1d_2[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " re_lu_3 (ReLU)              (None, 186, 48)              0         ['batch_normalization_2[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)           (None, 186, 64)              3072      ['re_lu_3[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_3 (Bat  (None, 186, 64)              256       ['conv1d_3[0][0]']            \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " dropout (Dropout)           (None, 186, 64)              0         ['batch_normalization_3[0][0]'\n",
      "                                                                    ]                             \n",
      "                                                                                                  \n",
      " add (Add)                   (None, 186, 64)              0         ['batch_normalization_1[0][0]'\n",
      "                                                                    , 'dropout[0][0]']            \n",
      "                                                                                                  \n",
      " re_lu_4 (ReLU)              (None, 186, 64)              0         ['add[0][0]']                 \n",
      "                                                                                                  \n",
      " one_to_two (OneToTwo)       (None, 186, 186, 64)         0         ['re_lu_4[0][0]']             \n",
      "                                                                                                  \n",
      " concat_dist2d (ConcatDist2  (None, 186, 186, 65)         0         ['one_to_two[0][0]']          \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " re_lu_5 (ReLU)              (None, 186, 186, 65)         0         ['concat_dist2d[0][0]']       \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)             (None, 186, 186, 48)         28080     ['re_lu_5[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_4 (Bat  (None, 186, 186, 48)         192       ['conv2d[0][0]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " symmetrize2d (Symmetrize2D  (None, 186, 186, 48)         0         ['batch_normalization_4[0][0]'\n",
      " )                                                                  ]                             \n",
      "                                                                                                  \n",
      " cropping2d (Cropping2D)     (None, 20, 20, 48)           0         ['symmetrize2d[0][0]']        \n",
      "                                                                                                  \n",
      " upper_tri (UpperTri)        (None, 210, 48)              0         ['cropping2d[0][0]']          \n",
      "                                                                                                  \n",
      " dense (Dense)               (None, 210, 2)               98        ['upper_tri[0][0]']           \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 817874 (3.12 MB)\n",
      "Trainable params: 817298 (3.12 MB)\n",
      "Non-trainable params: 576 (2.25 KB)\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "upsample_1d (None, 62, 256) (None, 372, 256)\n",
      "0.16666666666666666\n",
      "0.16666666666666666\n",
      "conv1d (None, 372, 256) (None, 372, 64)\n",
      "1.0\n",
      "0.16666666666666666\n",
      "max_pooling1d (None, 372, 64) (None, 186, 64)\n",
      "2.0\n",
      "0.3333333333333333\n",
      "conv1d_1 (None, 186, 64) (None, 186, 64)\n",
      "1.0\n",
      "0.3333333333333333\n",
      "conv1d_2 (None, 186, 64) (None, 186, 48)\n",
      "1.0\n",
      "0.3333333333333333\n",
      "conv1d_3 (None, 186, 48) (None, 186, 64)\n",
      "1.0\n",
      "0.3333333333333333\n",
      "conv2d (None, 186, 186, 65) (None, 186, 186, 48)\n",
      "1.0\n",
      "0.3333333333333333\n",
      "self.target_lengths[-1] 210\n",
      "target full length is 62\n",
      "model_strides [1]\n",
      "target_lengths [210]\n",
      "target_crops [-74]\n",
      "GPUs Available: 0\n",
      "WARNING: No GPUs detected, training will run on CPU!\n",
      "Hello\n",
      "We're in the IF portion\n",
      "Epoch 1/10000\n",
      "This is triu_tup (array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "       17, 17, 17, 18, 18, 19]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  8,  9, 10, 11,\n",
      "       12, 13, 14, 15, 16, 17, 18, 19,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 12, 13, 14, 15, 16, 17, 18, 19, 13, 14, 15, 16, 17,\n",
      "       18, 19, 14, 15, 16, 17, 18, 19, 15, 16, 17, 18, 19, 16, 17, 18, 19,\n",
      "       17, 18, 19, 18, 19, 19]))\n",
      "This is triu_tup (array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "       17, 17, 17, 18, 18, 19]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  8,  9, 10, 11,\n",
      "       12, 13, 14, 15, 16, 17, 18, 19,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 12, 13, 14, 15, 16, 17, 18, 19, 13, 14, 15, 16, 17,\n",
      "       18, 19, 14, 15, 16, 17, 18, 19, 15, 16, 17, 18, 19, 16, 17, 18, 19,\n",
      "       17, 18, 19, 18, 19, 19]))\n",
      "2025-03-08 02:08:25.555076: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80954640 exceeds 10% of free system memory.\n",
      "2025-03-08 02:08:25.555144: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80954640 exceeds 10% of free system memory.\n",
      " 1/41 [..............................] - ETA: 6:30 - loss: 0.3402 - pearsonr: 0.0139 - r2: -1.70202025-03-08 02:08:26.213900: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80954640 exceeds 10% of free system memory.\n",
      "2025-03-08 02:08:26.213962: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80954640 exceeds 10% of free system memory.\n",
      " 2/41 [>.............................] - ETA: 8s - loss: 0.3373 - pearsonr: 0.0369 - r2: -2.1701  2025-03-08 02:08:26.424013: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 80954640 exceeds 10% of free system memory.\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1649 - pearsonr: 0.0064 - r2: -0.4665This is triu_tup (array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "        0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "        1,  1,  1,  1,  1,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "        2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "        3,  3,  3,  3,  3,  3,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n",
      "        4,  4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
      "        5,  5,  5,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n",
      "        7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,\n",
      "        8,  8,  8,  8,  8,  8,  8,  8,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "        9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11, 11, 11, 11,\n",
      "       11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13, 13,\n",
      "       13, 13, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 16, 16, 16,\n",
      "       17, 17, 17, 18, 18, 19]), array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13,\n",
      "       14, 15, 16, 17, 18, 19,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "       15, 16, 17, 18, 19,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "       17, 18, 19,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,\n",
      "        7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19,  8,  9, 10, 11,\n",
      "       12, 13, 14, 15, 16, 17, 18, 19,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "       18, 19, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 11, 12, 13, 14, 15,\n",
      "       16, 17, 18, 19, 12, 13, 14, 15, 16, 17, 18, 19, 13, 14, 15, 16, 17,\n",
      "       18, 19, 14, 15, 16, 17, 18, 19, 15, 16, 17, 18, 19, 16, 17, 18, 19,\n",
      "       17, 18, 19, 18, 19, 19]))\n",
      "/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/keras/src/engine/training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n",
      "\n",
      "Epoch 1: val_pearsonr improved from -inf to 0.00675, saving model to ./data/1m/train_out/model_best.h5\n",
      "41/41 [==============================] - 20s 246ms/step - loss: 0.1649 - pearsonr: 0.0064 - r2: -0.4665 - val_loss: 0.1072 - val_pearsonr: 0.0067 - val_r2: -0.0816\n",
      "Epoch 2/10000\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1804 - pearsonr: -0.0014 - r2: -0.6239   \n",
      "Epoch 2: val_pearsonr did not improve from 0.00675\n",
      "41/41 [==============================] - 8s 188ms/step - loss: 0.1804 - pearsonr: -0.0014 - r2: -0.6239 - val_loss: 0.1540 - val_pearsonr: -0.0193 - val_r2: -0.5619\n",
      "Epoch 3/10000\n",
      "41/41 [==============================] - ETA: 0s - loss: 0.1493 - pearsonr: 0.0080 - r2: -0.4469\n",
      "Epoch 3: val_pearsonr improved from 0.00675 to 0.02468, saving model to ./data/1m/train_out/model_best.h5\n",
      "41/41 [==============================] - 8s 200ms/step - loss: 0.1493 - pearsonr: 0.0080 - r2: -0.4469 - val_loss: 0.1209 - val_pearsonr: 0.0247 - val_r2: -0.2250\n",
      "Epoch 4/10000\n",
      "37/41 [==========================>...] - ETA: 0s - loss: 0.1666 - pearsonr: -0.0277 - r2: -0.4420^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/017448899/basenji/bin/akita_train.py\", line 199, in <module>\n",
      "    main()\n",
      "  File \"/home/017448899/basenji/bin/akita_train.py\", line 187, in main\n",
      "    seqnn_trainer.fit_keras(seqnn_model)\n",
      "  File \"/home/017448899/basenji/basenji/trainer.py\", line 147, in fit_keras\n",
      "    seqnn_model.model.fit(\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/keras/src/utils/traceback_utils.py\", line 65, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/keras/src/engine/training.py\", line 1742, in fit\n",
      "    tmp_logs = self.train_function(iterator)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py\", line 150, in error_handler\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 825, in __call__\n",
      "    result = self._call(*args, **kwds)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\", line 857, in _call\n",
      "    return self._no_variable_creation_fn(*args, **kwds)  # pylint: disable=not-callable\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py\", line 148, in __call__\n",
      "    return concrete_function._call_flat(\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py\", line 1349, in _call_flat\n",
      "    return self._build_call_outputs(self._inference_function(*args))\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\", line 196, in __call__\n",
      "    outputs = self._bound_context.call_function(\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/context.py\", line 1457, in call_function\n",
      "    outputs = execute.execute(\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\", line 53, in quick_execute\n",
      "    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! /home/017448899/basenji/bin/akita_train.py -k -o ./data/1m/train_out/  ./data/1m/params_tutorial.json ./data/1m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-03-10 18:34:20.771359: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-10 18:34:22.511286: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/017448899/basenji/bin/akita_train.py\", line 27, in <module>\n",
      "    import tensorflow as tf\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/__init__.py\", line 38, in <module>\n",
      "    from tensorflow.python.tools import module_util as _module_util\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/__init__.py\", line 45, in <module>\n",
      "    from tensorflow.python.feature_column import feature_column_lib as feature_column\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column_lib.py\", line 18, in <module>\n",
      "    from tensorflow.python.feature_column.feature_column import *\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/feature_column/feature_column.py\", line 143, in <module>\n",
      "    from tensorflow.python.layers import base\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/layers/base.py\", line 16, in <module>\n",
      "^C\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/keras/__init__.py\", line 25, in <module>\n",
      "    from tensorflow.python.keras import models\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/keras/models.py\", line 22, in <module>\n",
      "    from tensorflow.python.keras.engine import functional\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/keras/engine/functional.py\", line 32, in <module>\n",
      "    from tensorflow.python.keras.engine import training as training_lib\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\", line 43, in <module>\n",
      "    from tensorflow.python.keras import callbacks as callbacks_module\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/tensorflow/python/keras/callbacks.py\", line 68, in <module>\n",
      "    import requests\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/requests/__init__.py\", line 45, in <module>\n",
      "    from .exceptions import RequestsDependencyWarning\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/requests/exceptions.py\", line 9, in <module>\n",
      "    from .compat import JSONDecodeError as CompatJSONDecodeError\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/requests/compat.py\", line 30, in <module>\n",
      "    chardet = _resolve_char_detection()\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/requests/compat.py\", line 24, in _resolve_char_detection\n",
      "    chardet = importlib.import_module(lib)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/importlib/__init__.py\", line 127, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/charset_normalizer/__init__.py\", line 24, in <module>\n",
      "    from .api import from_bytes, from_fp, from_path, is_binary\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/charset_normalizer/api.py\", line 5, in <module>\n",
      "    from .cd import (\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/charset_normalizer/cd.py\", line 14, in <module>\n",
      "    from .md import is_suspiciously_successive_range\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/charset_normalizer/md.py\", line 385, in <module>\n",
      "    class ArchaicUpperLowerPlugin(MessDetectorPlugin):\n",
      "  File \"/home/017448899/miniconda3/envs/basenji/lib/python3.8/site-packages/charset_normalizer/md.py\", line 444, in ArchaicUpperLowerPlugin\n",
      "    def reset(self) -> None:  # pragma: no cover\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! /home/017448899/basenji/bin/akita_train.py -k -o ./data/1m_onehot/train_out/  ./data/1m_onehot/params_tutorial.json ./data/1m_onehot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See explore_model.ipynb for tips on investigating the output of a trained model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basenji",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
